{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f900742c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0552a9f",
   "metadata": {},
   "source": [
    "CREATE DATA ON LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba306de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# QA\n",
    "inputs = [\n",
    "    \"For customer-facing applications, which company's models dominate the top rankings?\",\n",
    "    \"What percentage of respondents are using RAG in some form?\",\n",
    "    \"How often are most respondents updating their models?\",\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"OpenAI models dominate, with 3 of the top 5 and half of the top 10 most popular models for customer-facing apps.\",\n",
    "    \"70% of respondents are using RAG in some form.\",\n",
    "    \"More than 50% update their models at least monthly, with 17% doing so weekly.\",\n",
    "]\n",
    "\n",
    "# Dataset\n",
    "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]\n",
    "df = pd.DataFrame(qa_pairs)\n",
    "\n",
    "# Write to csv\n",
    "csv_path = \"../data/goldens.csv\"\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c1e34b",
   "metadata": {},
   "source": [
    "ADD DATA TO LANGSMITH - to evaluate the RAG APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d45b9763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['43edae04-5b8d-4824-89c4-f011d7e3ff85',\n",
       "  '9820c451-b433-4091-b79a-4042b2244967',\n",
       "  'dfa4efb5-5bd9-4a90-9b49-459b6e4a0646'],\n",
       " 'count': 3}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"LLMOPS_Dataset\"\n",
    "\n",
    "# Store\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Input and expected output pairs for LLMOPS evaluations\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in inputs],\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18500ad6",
   "metadata": {},
   "source": [
    "Run The RAG APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edcd3b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # Add parent directory to path to find multi_doc_chat module\n",
    "\n",
    "from pathlib import Path\n",
    "from multi_doc_chat.src.document_ingestion.data_ingestion import ChatIngestor\n",
    "from multi_doc_chat.src.document_chat.retrieval import ConversationalRAG\n",
    "import os\n",
    "\n",
    "# Simple file adapter for local file paths\n",
    "class LocalFileAdapter:\n",
    "    \"\"\"Adapter for local file paths to work with ChatIngestor.\"\"\"\n",
    "    def __init__(self, file_path: str):\n",
    "        self.path = Path(file_path)\n",
    "        self.name = self.path.name\n",
    "    \n",
    "    def getbuffer(self) -> bytes:\n",
    "        return self.path.read_bytes()\n",
    "\n",
    "\n",
    "def answer_ai_report_question(\n",
    "    inputs: dict,\n",
    "    data_path: str = \"../data/The 2025 AI Engineering Report.txt\",\n",
    "    chunk_size: int = 1000,\n",
    "    chunk_overlap: int = 200,\n",
    "    k: int = 5\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Answer questions about the AI Engineering Report using RAG.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Dictionary containing the question, e.g., {\"question\": \"What is RAG?\"}\n",
    "        data_path: Path to the AI Engineering Report text file\n",
    "        chunk_size: Size of text chunks for splitting\n",
    "        chunk_overlap: Overlap between chunks\n",
    "        k: Number of documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with the answer, e.g., {\"answer\": \"RAG stands for...\"}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract question from inputs\n",
    "        question = inputs.get(\"question\", \"\")\n",
    "        if not question:\n",
    "            return {\"answer\": \"No question provided\"}\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not Path(data_path).exists():\n",
    "            return {\"answer\": f\"Data file not found: {data_path}\"}\n",
    "        \n",
    "        # Create file adapter\n",
    "        file_adapter = LocalFileAdapter(data_path)\n",
    "        \n",
    "        # Build index using ChatIngestor\n",
    "        ingestor = ChatIngestor(\n",
    "            temp_base=\"data\",\n",
    "            faiss_base=\"faiss_index\",\n",
    "            use_session_dirs=True\n",
    "        )\n",
    "        \n",
    "        # Build retriever\n",
    "        ingestor.built_retriver(\n",
    "            uploaded_files=[file_adapter],\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            k=k\n",
    "        )\n",
    "        \n",
    "        # Get session ID and index path\n",
    "        session_id = ingestor.session_id\n",
    "        index_path = f\"faiss_index/{session_id}\"\n",
    "        \n",
    "        # Create RAG instance and load retriever\n",
    "        rag = ConversationalRAG(session_id=session_id)\n",
    "        rag.load_retriever_from_faiss(\n",
    "            index_path=index_path,\n",
    "            k=k,\n",
    "            index_name=os.getenv(\"FAISS_INDEX_NAME\", \"index\")\n",
    "        )\n",
    "        \n",
    "        # Get answer\n",
    "        answer = rag.invoke(question, chat_history=[])\n",
    "        \n",
    "        return {\"answer\": answer}\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"answer\": f\"Error: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cedcf7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"timestamp\": \"2026-01-04T13:14:38.581178Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:14:38.581726Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:14:38.582159Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:14:38.582751Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:14:38.586562Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20260104_171438_7a3f5797\", \"temp_dir\": \"data/session_20260104_171438_7a3f5797\", \"faiss_dir\": \"faiss_index/session_20260104_171438_7a3f5797\", \"sessionized\": true, \"timestamp\": \"2026-01-04T13:14:38.587624Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"The 2025 AI Engineering Report.txt\", \"saved_as\": \"data/session_20260104_171438_7a3f5797/0ef0ac20.txt\", \"timestamp\": \"2026-01-04T13:14:38.588824Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"timestamp\": \"2026-01-04T13:14:38.581726Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:14:38.582159Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:14:38.582751Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:14:38.586562Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20260104_171438_7a3f5797\", \"temp_dir\": \"data/session_20260104_171438_7a3f5797\", \"faiss_dir\": \"faiss_index/session_20260104_171438_7a3f5797\", \"sessionized\": true, \"timestamp\": \"2026-01-04T13:14:38.587624Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"The 2025 AI Engineering Report.txt\", \"saved_as\": \"data/session_20260104_171438_7a3f5797/0ef0ac20.txt\", \"timestamp\": \"2026-01-04T13:14:38.588824Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2026-01-04T13:14:38.589681Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 10, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2026-01-04T13:14:38.590525Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:14:38.591232Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"count\": 1, \"timestamp\": \"2026-01-04T13:14:38.589681Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 10, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2026-01-04T13:14:38.590525Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:14:38.591232Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "/Users/s.kumar/Documents/learn/LLMOPS/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/s.kumar/Documents/learn/LLMOPS/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Loading faiss.\n",
      "Loading faiss.\n",
      "Successfully loaded faiss.\n",
      "Successfully loaded faiss.\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20260104_171438_7a3f5797\", \"timestamp\": \"2026-01-04T13:14:46.785001Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2026-01-04T13:14:46.785357Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2026-01-04T13:14:46.786550Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:14:46.786711Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:14:46.786889Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:14:46.787136Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:14:46.788514Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"groq\", \"model\": \"openai/gpt-oss-20b\", \"timestamp\": \"2026-01-04T13:14:46.788692Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20260104_171438_7a3f5797\", \"timestamp\": \"2026-01-04T13:14:46.785001Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2026-01-04T13:14:46.785357Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2026-01-04T13:14:46.786550Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:14:46.786711Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:14:46.786889Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:14:46.787136Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:14:46.788514Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"groq\", \"model\": \"openai/gpt-oss-20b\", \"timestamp\": \"2026-01-04T13:14:46.788692Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20260104_171438_7a3f5797\", \"timestamp\": \"2026-01-04T13:14:46.876204Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20260104_171438_7a3f5797\", \"timestamp\": \"2026-01-04T13:14:46.876567Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2026-01-04T13:14:46.877386Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:14:46.877535Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:14:46.877752Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:14:46.877893Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:14:46.878999Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:14:46.879179Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"session_id\": \"session_20260104_171438_7a3f5797\", \"timestamp\": \"2026-01-04T13:14:46.876204Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20260104_171438_7a3f5797\", \"timestamp\": \"2026-01-04T13:14:46.876567Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2026-01-04T13:14:46.877386Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:14:46.877535Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:14:46.877752Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:14:46.877893Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:14:46.878999Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:14:46.879179Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"session_id\": \"session_20260104_171438_7a3f5797\", \"timestamp\": \"2026-01-04T13:14:49.910824Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20260104_171438_7a3f5797\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20260104_171438_7a3f5797\", \"timestamp\": \"2026-01-04T13:14:49.911432Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "{\"session_id\": \"session_20260104_171438_7a3f5797\", \"timestamp\": \"2026-01-04T13:14:49.910824Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20260104_171438_7a3f5797\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20260104_171438_7a3f5797\", \"timestamp\": \"2026-01-04T13:14:49.911432Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20260104_171438_7a3f5797\", \"user_input\": \"For customer-facing applications, which company's models dominate the top rankings?\", \"answer_preview\": \"OpenAI\\u2019s models dominate the top rankings for customer\\u2011facing applications.\", \"timestamp\": \"2026-01-04T13:14:50.854878Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20260104_171438_7a3f5797\", \"user_input\": \"For customer-facing applications, which company's models dominate the top rankings?\", \"answer_preview\": \"OpenAI\\u2019s models dominate the top rankings for customer\\u2011facing applications.\", \"timestamp\": \"2026-01-04T13:14:50.854878Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: For customer-facing applications, which company's models dominate the top rankings?\n",
      "\n",
      "Answer: OpenAI’s models dominate the top rankings for customer‑facing applications.\n"
     ]
    }
   ],
   "source": [
    "# Test the function with a sample question\n",
    "test_input = {\"question\": \"For customer-facing applications, which company's models dominate the top rankings?\"}\n",
    "result = answer_ai_report_question(test_input)\n",
    "print(\"Question:\", test_input[\"question\"])\n",
    "print(\"\\nAnswer:\", result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23800f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "181ec5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"timestamp\": \"2026-01-04T13:16:37.527733Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:37.528284Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:37.528681Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:16:37.529158Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:16:37.532419Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20260104_171637_ed860953\", \"temp_dir\": \"data/session_20260104_171637_ed860953\", \"faiss_dir\": \"faiss_index/session_20260104_171637_ed860953\", \"sessionized\": true, \"timestamp\": \"2026-01-04T13:16:37.534016Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"The 2025 AI Engineering Report.txt\", \"saved_as\": \"data/session_20260104_171637_ed860953/51ced838.txt\", \"timestamp\": \"2026-01-04T13:16:37.535222Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2026-01-04T13:16:37.535723Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:37.528284Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:37.528681Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:16:37.529158Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:16:37.532419Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20260104_171637_ed860953\", \"temp_dir\": \"data/session_20260104_171637_ed860953\", \"faiss_dir\": \"faiss_index/session_20260104_171637_ed860953\", \"sessionized\": true, \"timestamp\": \"2026-01-04T13:16:37.534016Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"The 2025 AI Engineering Report.txt\", \"saved_as\": \"data/session_20260104_171637_ed860953/51ced838.txt\", \"timestamp\": \"2026-01-04T13:16:37.535222Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2026-01-04T13:16:37.535723Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 10, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2026-01-04T13:16:37.536136Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:16:37.536518Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"chunks\": 10, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2026-01-04T13:16:37.536136Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:16:37.536518Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing all questions from the dataset:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"added\": 1, \"index\": \"faiss_index/session_20260104_171637_ed860953\", \"timestamp\": \"2026-01-04T13:16:41.099349Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2026-01-04T13:16:41.099751Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:41.100825Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:41.101049Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:41.101282Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:16:41.101541Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:16:41.102943Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"groq\", \"model\": \"openai/gpt-oss-20b\", \"timestamp\": \"2026-01-04T13:16:41.103151Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2026-01-04T13:16:41.099751Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:41.100825Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:41.101049Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:41.101282Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:16:41.101541Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:16:41.102943Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"groq\", \"model\": \"openai/gpt-oss-20b\", \"timestamp\": \"2026-01-04T13:16:41.103151Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20260104_171637_ed860953\", \"timestamp\": \"2026-01-04T13:16:41.132158Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20260104_171637_ed860953\", \"timestamp\": \"2026-01-04T13:16:41.132588Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:41.133491Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:41.133698Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:41.133922Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:16:41.134159Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:16:41.135409Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:16:41.135589Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"session_id\": \"session_20260104_171637_ed860953\", \"timestamp\": \"2026-01-04T13:16:41.132158Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20260104_171637_ed860953\", \"timestamp\": \"2026-01-04T13:16:41.132588Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:41.133491Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:41.133698Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:41.133922Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:16:41.134159Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:16:41.135409Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:16:41.135589Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"session_id\": \"session_20260104_171637_ed860953\", \"timestamp\": \"2026-01-04T13:16:44.209094Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20260104_171637_ed860953\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20260104_171637_ed860953\", \"timestamp\": \"2026-01-04T13:16:44.209692Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "{\"session_id\": \"session_20260104_171637_ed860953\", \"timestamp\": \"2026-01-04T13:16:44.209094Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20260104_171637_ed860953\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20260104_171637_ed860953\", \"timestamp\": \"2026-01-04T13:16:44.209692Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20260104_171637_ed860953\", \"user_input\": \"For customer-facing applications, which company's models dominate the top rankings?\", \"answer_preview\": \"OpenAI\\u2019s models dominate the top rankings for customer\\u2011facing applications.\", \"timestamp\": \"2026-01-04T13:16:45.213514Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:45.215844Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:45.216322Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:45.216776Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:16:45.217246Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20260104_171637_ed860953\", \"user_input\": \"For customer-facing applications, which company's models dominate the top rankings?\", \"answer_preview\": \"OpenAI\\u2019s models dominate the top rankings for customer\\u2011facing applications.\", \"timestamp\": \"2026-01-04T13:16:45.213514Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:45.215844Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:45.216322Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:45.216776Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:16:45.217246Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:16:45.220772Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20260104_171645_460d953b\", \"temp_dir\": \"data/session_20260104_171645_460d953b\", \"faiss_dir\": \"faiss_index/session_20260104_171645_460d953b\", \"sessionized\": true, \"timestamp\": \"2026-01-04T13:16:45.222233Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"The 2025 AI Engineering Report.txt\", \"saved_as\": \"data/session_20260104_171645_460d953b/36de2a20.txt\", \"timestamp\": \"2026-01-04T13:16:45.223823Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2026-01-04T13:16:45.225044Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 10, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2026-01-04T13:16:45.225729Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:16:45.226289Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:16:45.220772Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20260104_171645_460d953b\", \"temp_dir\": \"data/session_20260104_171645_460d953b\", \"faiss_dir\": \"faiss_index/session_20260104_171645_460d953b\", \"sessionized\": true, \"timestamp\": \"2026-01-04T13:16:45.222233Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"The 2025 AI Engineering Report.txt\", \"saved_as\": \"data/session_20260104_171645_460d953b/36de2a20.txt\", \"timestamp\": \"2026-01-04T13:16:45.223823Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2026-01-04T13:16:45.225044Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 10, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2026-01-04T13:16:45.225729Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:16:45.226289Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: For customer-facing applications, which company's models dominate the top rankings?\n",
      "A1: OpenAI’s models dominate the top rankings for customer‑facing applications.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"added\": 1, \"index\": \"faiss_index/session_20260104_171645_460d953b\", \"timestamp\": \"2026-01-04T13:16:48.475303Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2026-01-04T13:16:48.475832Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:48.477255Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:48.477615Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:48.477937Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:16:48.478347Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:16:48.480234Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"groq\", \"model\": \"openai/gpt-oss-20b\", \"timestamp\": \"2026-01-04T13:16:48.480522Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2026-01-04T13:16:48.475832Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:48.477255Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:48.477615Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:48.477937Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:16:48.478347Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:16:48.480234Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"groq\", \"model\": \"openai/gpt-oss-20b\", \"timestamp\": \"2026-01-04T13:16:48.480522Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20260104_171645_460d953b\", \"timestamp\": \"2026-01-04T13:16:48.514720Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20260104_171645_460d953b\", \"timestamp\": \"2026-01-04T13:16:48.515138Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:48.516169Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:48.516324Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:48.516484Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:16:48.516632Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:16:48.517896Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:16:48.518076Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"session_id\": \"session_20260104_171645_460d953b\", \"timestamp\": \"2026-01-04T13:16:48.514720Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20260104_171645_460d953b\", \"timestamp\": \"2026-01-04T13:16:48.515138Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:48.516169Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:48.516324Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:48.516484Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:16:48.516632Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:16:48.517896Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:16:48.518076Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"session_id\": \"session_20260104_171645_460d953b\", \"timestamp\": \"2026-01-04T13:16:52.092861Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20260104_171645_460d953b\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20260104_171645_460d953b\", \"timestamp\": \"2026-01-04T13:16:52.093452Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "{\"session_id\": \"session_20260104_171645_460d953b\", \"timestamp\": \"2026-01-04T13:16:52.092861Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20260104_171645_460d953b\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20260104_171645_460d953b\", \"timestamp\": \"2026-01-04T13:16:52.093452Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20260104_171645_460d953b\", \"user_input\": \"What percentage of respondents are using RAG in some form?\", \"answer_preview\": \"70% of respondents are using RAG in some form.\", \"timestamp\": \"2026-01-04T13:16:52.777477Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:52.780018Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:52.780536Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:52.780965Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:16:52.781406Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:16:52.784596Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20260104_171645_460d953b\", \"user_input\": \"What percentage of respondents are using RAG in some form?\", \"answer_preview\": \"70% of respondents are using RAG in some form.\", \"timestamp\": \"2026-01-04T13:16:52.777477Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:52.780018Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:52.780536Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:52.780965Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:16:52.781406Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:16:52.784596Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20260104_171652_5a603a38\", \"temp_dir\": \"data/session_20260104_171652_5a603a38\", \"faiss_dir\": \"faiss_index/session_20260104_171652_5a603a38\", \"sessionized\": true, \"timestamp\": \"2026-01-04T13:16:52.785949Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"The 2025 AI Engineering Report.txt\", \"saved_as\": \"data/session_20260104_171652_5a603a38/f1bd6d58.txt\", \"timestamp\": \"2026-01-04T13:16:52.787408Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2026-01-04T13:16:52.790695Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 10, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2026-01-04T13:16:52.791429Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:16:52.792221Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"session_id\": \"session_20260104_171652_5a603a38\", \"temp_dir\": \"data/session_20260104_171652_5a603a38\", \"faiss_dir\": \"faiss_index/session_20260104_171652_5a603a38\", \"sessionized\": true, \"timestamp\": \"2026-01-04T13:16:52.785949Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"The 2025 AI Engineering Report.txt\", \"saved_as\": \"data/session_20260104_171652_5a603a38/f1bd6d58.txt\", \"timestamp\": \"2026-01-04T13:16:52.787408Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2026-01-04T13:16:52.790695Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 10, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2026-01-04T13:16:52.791429Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:16:52.792221Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2: What percentage of respondents are using RAG in some form?\n",
      "A2: 70% of respondents are using RAG in some form.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"added\": 1, \"index\": \"faiss_index/session_20260104_171652_5a603a38\", \"timestamp\": \"2026-01-04T13:16:56.091058Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2026-01-04T13:16:56.091520Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:56.092788Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:56.093098Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:56.093414Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:16:56.093707Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:16:56.095691Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"groq\", \"model\": \"openai/gpt-oss-20b\", \"timestamp\": \"2026-01-04T13:16:56.095956Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2026-01-04T13:16:56.091520Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:56.092788Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:56.093098Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:56.093414Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:16:56.093707Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:16:56.095691Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"groq\", \"model\": \"openai/gpt-oss-20b\", \"timestamp\": \"2026-01-04T13:16:56.095956Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20260104_171652_5a603a38\", \"timestamp\": \"2026-01-04T13:16:56.128318Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20260104_171652_5a603a38\", \"timestamp\": \"2026-01-04T13:16:56.128753Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:56.129642Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:56.129804Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:56.129956Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:16:56.130138Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:16:56.131663Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:16:56.131895Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"session_id\": \"session_20260104_171652_5a603a38\", \"timestamp\": \"2026-01-04T13:16:56.128318Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20260104_171652_5a603a38\", \"timestamp\": \"2026-01-04T13:16:56.128753Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:56.129642Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:56.129804Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:16:56.129956Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:16:56.130138Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:16:56.131663Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:16:56.131895Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"session_id\": \"session_20260104_171652_5a603a38\", \"timestamp\": \"2026-01-04T13:16:59.450858Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20260104_171652_5a603a38\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20260104_171652_5a603a38\", \"timestamp\": \"2026-01-04T13:16:59.451483Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "{\"session_id\": \"session_20260104_171652_5a603a38\", \"timestamp\": \"2026-01-04T13:16:59.450858Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20260104_171652_5a603a38\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20260104_171652_5a603a38\", \"timestamp\": \"2026-01-04T13:16:59.451483Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20260104_171652_5a603a38\", \"user_input\": \"How often are most respondents updating their models?\", \"answer_preview\": \"Most respondents update their models on a monthly basis.\", \"timestamp\": \"2026-01-04T13:17:00.187117Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20260104_171652_5a603a38\", \"user_input\": \"How often are most respondents updating their models?\", \"answer_preview\": \"Most respondents update their models on a monthly basis.\", \"timestamp\": \"2026-01-04T13:17:00.187117Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3: How often are most respondents updating their models?\n",
      "A3: Most respondents update their models on a monthly basis.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Test with all golden questions\n",
    "print(\"Testing all questions from the dataset:\\n\")\n",
    "for i, q in enumerate(inputs, 1):\n",
    "    test_input = {\"question\": q}\n",
    "    result = answer_ai_report_question(test_input)\n",
    "    print(f\"Q{i}: {q}\")\n",
    "    print(f\"A{i}: {result['answer']}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d74edf7",
   "metadata": {},
   "source": [
    "Prebuilt Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0154544b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1761230954.908546  392834 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-agenticAIReport-qa-rag-gemini-85cbad80' at:\n",
      "https://smith.langchain.com/o/5ab3a087-cdc4-52b4-b7ec-104866d876b9/datasets/0906187c-3089-4bae-98a1-bf67dba8caf6/compare?selectedSessions=c0917fa7-950b-49d8-ad0b-1298e7f4f9e2\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]{\"timestamp\": \"2025-10-23T14:49:17.249961Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:17.250718Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:17.251276Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-23T14:49:17.251777Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:17.249961Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:17.250718Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:17.251276Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-23T14:49:17.251777Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-23T14:49:17.255373Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20251023_184917_f1e4d16d\", \"temp_dir\": \"data/session_20251023_184917_f1e4d16d\", \"faiss_dir\": \"faiss_index/session_20251023_184917_f1e4d16d\", \"sessionized\": true, \"timestamp\": \"2025-10-23T14:49:17.257023Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"The 2025 AI Engineering Report.txt\", \"saved_as\": \"data/session_20251023_184917_f1e4d16d/67c96c86.txt\", \"timestamp\": \"2025-10-23T14:49:17.258582Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-10-23T14:49:17.260178Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 10, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-10-23T14:49:17.261104Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2025-10-23T14:49:17.262470Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-23T14:49:17.255373Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20251023_184917_f1e4d16d\", \"temp_dir\": \"data/session_20251023_184917_f1e4d16d\", \"faiss_dir\": \"faiss_index/session_20251023_184917_f1e4d16d\", \"sessionized\": true, \"timestamp\": \"2025-10-23T14:49:17.257023Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"The 2025 AI Engineering Report.txt\", \"saved_as\": \"data/session_20251023_184917_f1e4d16d/67c96c86.txt\", \"timestamp\": \"2025-10-23T14:49:17.258582Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-10-23T14:49:17.260178Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 10, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-10-23T14:49:17.261104Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2025-10-23T14:49:17.262470Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20251023_184917_f1e4d16d\", \"timestamp\": \"2025-10-23T14:49:20.282878Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-10-23T14:49:20.283317Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:20.284149Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:20.284311Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:20.284445Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-23T14:49:20.284567Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-23T14:49:20.285801Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.0-flash\", \"timestamp\": \"2025-10-23T14:49:20.285970Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251023_184917_f1e4d16d\", \"timestamp\": \"2025-10-23T14:49:20.287667Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "E0000 00:00:1761230960.287434  392834 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "{\"session_id\": \"session_20251023_184917_f1e4d16d\", \"timestamp\": \"2025-10-23T14:49:20.287997Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:20.288709Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:20.288855Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:20.289020Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-23T14:49:20.289205Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-23T14:49:20.290315Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2025-10-23T14:49:20.290449Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20251023_184917_f1e4d16d\", \"timestamp\": \"2025-10-23T14:49:20.282878Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-10-23T14:49:20.283317Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:20.284149Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:20.284311Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:20.284445Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-23T14:49:20.284567Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-23T14:49:20.285801Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.0-flash\", \"timestamp\": \"2025-10-23T14:49:20.285970Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251023_184917_f1e4d16d\", \"timestamp\": \"2025-10-23T14:49:20.287667Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "E0000 00:00:1761230960.287434  392834 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "{\"session_id\": \"session_20251023_184917_f1e4d16d\", \"timestamp\": \"2025-10-23T14:49:20.287997Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:20.288709Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:20.288855Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:20.289020Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-23T14:49:20.289205Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-23T14:49:20.290315Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2025-10-23T14:49:20.290449Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"session_id\": \"session_20251023_184917_f1e4d16d\", \"timestamp\": \"2025-10-23T14:49:23.151820Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251023_184917_f1e4d16d\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251023_184917_f1e4d16d\", \"timestamp\": \"2025-10-23T14:49:23.152433Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "{\"session_id\": \"session_20251023_184917_f1e4d16d\", \"timestamp\": \"2025-10-23T14:49:23.151820Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251023_184917_f1e4d16d\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251023_184917_f1e4d16d\", \"timestamp\": \"2025-10-23T14:49:23.152433Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "{\"session_id\": \"session_20251023_184917_f1e4d16d\", \"user_input\": \"What percentage of respondents are using RAG in some form?\", \"answer_preview\": \"According to the survey, 70% of respondents are using RAG in some form. This demonstrates the technology's effectiveness in addressing limitations of \", \"timestamp\": \"2025-10-23T14:49:30.580689Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "{\"session_id\": \"session_20251023_184917_f1e4d16d\", \"user_input\": \"What percentage of respondents are using RAG in some form?\", \"answer_preview\": \"According to the survey, 70% of respondents are using RAG in some form. This demonstrates the technology's effectiveness in addressing limitations of \", \"timestamp\": \"2025-10-23T14:49:30.580689Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised NotFound: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised NotFound: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..\n",
      "1it [00:16, 16.23s/it]{\"timestamp\": \"2025-10-23T14:49:33.477852Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:33.478337Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:33.478648Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-23T14:49:33.478940Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-23T14:49:33.481067Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20251023_184933_5d7a9f2f\", \"temp_dir\": \"data/session_20251023_184933_5d7a9f2f\", \"faiss_dir\": \"faiss_index/session_20251023_184933_5d7a9f2f\", \"sessionized\": true, \"timestamp\": \"2025-10-23T14:49:33.482204Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"The 2025 AI Engineering Report.txt\", \"saved_as\": \"data/session_20251023_184933_5d7a9f2f/9be5fd57.txt\", \"timestamp\": \"2025-10-23T14:49:33.483290Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-10-23T14:49:33.483792Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 10, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-10-23T14:49:33.484162Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2025-10-23T14:49:33.484480Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "1it [00:16, 16.23s/it]{\"timestamp\": \"2025-10-23T14:49:33.477852Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:33.478337Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:33.478648Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-23T14:49:33.478940Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-23T14:49:33.481067Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20251023_184933_5d7a9f2f\", \"temp_dir\": \"data/session_20251023_184933_5d7a9f2f\", \"faiss_dir\": \"faiss_index/session_20251023_184933_5d7a9f2f\", \"sessionized\": true, \"timestamp\": \"2025-10-23T14:49:33.482204Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"The 2025 AI Engineering Report.txt\", \"saved_as\": \"data/session_20251023_184933_5d7a9f2f/9be5fd57.txt\", \"timestamp\": \"2025-10-23T14:49:33.483290Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-10-23T14:49:33.483792Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 10, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-10-23T14:49:33.484162Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2025-10-23T14:49:33.484480Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20251023_184933_5d7a9f2f\", \"timestamp\": \"2025-10-23T14:49:36.428186Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-10-23T14:49:36.428709Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:36.430018Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:36.430357Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:36.430632Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-23T14:49:36.431029Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-23T14:49:36.432939Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.0-flash\", \"timestamp\": \"2025-10-23T14:49:36.433219Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251023_184933_5d7a9f2f\", \"timestamp\": \"2025-10-23T14:49:36.434847Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "E0000 00:00:1761230976.434647  392834 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "{\"session_id\": \"session_20251023_184933_5d7a9f2f\", \"timestamp\": \"2025-10-23T14:49:36.435174Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:36.436110Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:36.436367Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:36.436615Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-23T14:49:36.436800Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-23T14:49:36.438301Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2025-10-23T14:49:36.438482Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20251023_184933_5d7a9f2f\", \"timestamp\": \"2025-10-23T14:49:36.428186Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-10-23T14:49:36.428709Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:36.430018Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:36.430357Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:36.430632Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-23T14:49:36.431029Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-23T14:49:36.432939Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.0-flash\", \"timestamp\": \"2025-10-23T14:49:36.433219Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251023_184933_5d7a9f2f\", \"timestamp\": \"2025-10-23T14:49:36.434847Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "E0000 00:00:1761230976.434647  392834 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "{\"session_id\": \"session_20251023_184933_5d7a9f2f\", \"timestamp\": \"2025-10-23T14:49:36.435174Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:36.436110Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:36.436367Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:36.436615Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-23T14:49:36.436800Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-23T14:49:36.438301Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2025-10-23T14:49:36.438482Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"session_id\": \"session_20251023_184933_5d7a9f2f\", \"timestamp\": \"2025-10-23T14:49:39.388222Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251023_184933_5d7a9f2f\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251023_184933_5d7a9f2f\", \"timestamp\": \"2025-10-23T14:49:39.388628Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "{\"session_id\": \"session_20251023_184933_5d7a9f2f\", \"timestamp\": \"2025-10-23T14:49:39.388222Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251023_184933_5d7a9f2f\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251023_184933_5d7a9f2f\", \"timestamp\": \"2025-10-23T14:49:39.388628Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "{\"session_id\": \"session_20251023_184933_5d7a9f2f\", \"user_input\": \"For customer-facing applications, which company's models dominate the top rankings?\", \"answer_preview\": \"For customer-facing applications, OpenAI models dominate the top rankings. They hold 3 of the top 5 positions and represent half of the top 10 most po\", \"timestamp\": \"2025-10-23T14:49:42.869440Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "{\"session_id\": \"session_20251023_184933_5d7a9f2f\", \"user_input\": \"For customer-facing applications, which company's models dominate the top rankings?\", \"answer_preview\": \"For customer-facing applications, OpenAI models dominate the top rankings. They hold 3 of the top 5 positions and represent half of the top 10 most po\", \"timestamp\": \"2025-10-23T14:49:42.869440Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised NotFound: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised NotFound: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..\n",
      "2it [00:27, 13.58s/it]{\"timestamp\": \"2025-10-23T14:49:45.207987Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:45.208497Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:45.208995Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-23T14:49:45.209414Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-23T14:49:45.211582Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20251023_184945_7e656532\", \"temp_dir\": \"data/session_20251023_184945_7e656532\", \"faiss_dir\": \"faiss_index/session_20251023_184945_7e656532\", \"sessionized\": true, \"timestamp\": \"2025-10-23T14:49:45.212536Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"The 2025 AI Engineering Report.txt\", \"saved_as\": \"data/session_20251023_184945_7e656532/83454660.txt\", \"timestamp\": \"2025-10-23T14:49:45.213638Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-10-23T14:49:45.214181Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 10, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-10-23T14:49:45.214599Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2025-10-23T14:49:45.214932Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "2it [00:27, 13.58s/it]{\"timestamp\": \"2025-10-23T14:49:45.207987Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:45.208497Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:45.208995Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-23T14:49:45.209414Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-23T14:49:45.211582Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20251023_184945_7e656532\", \"temp_dir\": \"data/session_20251023_184945_7e656532\", \"faiss_dir\": \"faiss_index/session_20251023_184945_7e656532\", \"sessionized\": true, \"timestamp\": \"2025-10-23T14:49:45.212536Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"The 2025 AI Engineering Report.txt\", \"saved_as\": \"data/session_20251023_184945_7e656532/83454660.txt\", \"timestamp\": \"2025-10-23T14:49:45.213638Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2025-10-23T14:49:45.214181Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 10, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-10-23T14:49:45.214599Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2025-10-23T14:49:45.214932Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20251023_184945_7e656532\", \"timestamp\": \"2025-10-23T14:49:48.111154Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-10-23T14:49:48.111538Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:48.112527Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:48.112797Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:48.113028Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-23T14:49:48.113187Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-23T14:49:48.114526Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.0-flash\", \"timestamp\": \"2025-10-23T14:49:48.114728Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251023_184945_7e656532\", \"timestamp\": \"2025-10-23T14:49:48.116057Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "E0000 00:00:1761230988.115868  392834 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "{\"session_id\": \"session_20251023_184945_7e656532\", \"timestamp\": \"2025-10-23T14:49:48.116258Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:48.116966Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:48.117099Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:48.117266Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-23T14:49:48.117461Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-23T14:49:48.118585Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2025-10-23T14:49:48.118770Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20251023_184945_7e656532\", \"timestamp\": \"2025-10-23T14:49:48.111154Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-10-23T14:49:48.111538Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:48.112527Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:48.112797Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:48.113028Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-23T14:49:48.113187Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-23T14:49:48.114526Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"google\", \"model\": \"gemini-2.0-flash\", \"timestamp\": \"2025-10-23T14:49:48.114728Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251023_184945_7e656532\", \"timestamp\": \"2025-10-23T14:49:48.116057Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "E0000 00:00:1761230988.115868  392834 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "{\"session_id\": \"session_20251023_184945_7e656532\", \"timestamp\": \"2025-10-23T14:49:48.116258Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:48.116966Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:48.117099Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-23T14:49:48.117266Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-23T14:49:48.117461Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-23T14:49:48.118585Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2025-10-23T14:49:48.118770Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"session_id\": \"session_20251023_184945_7e656532\", \"timestamp\": \"2025-10-23T14:49:50.995118Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251023_184945_7e656532\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251023_184945_7e656532\", \"timestamp\": \"2025-10-23T14:49:50.995648Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "{\"session_id\": \"session_20251023_184945_7e656532\", \"timestamp\": \"2025-10-23T14:49:50.995118Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251023_184945_7e656532\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251023_184945_7e656532\", \"timestamp\": \"2025-10-23T14:49:50.995648Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "{\"session_id\": \"session_20251023_184945_7e656532\", \"user_input\": \"How often are most respondents updating their models?\", \"answer_preview\": \"More than 50% of organizations update their models at least monthly, with 17% performing updates on a weekly basis. This aggressive update schedule re\", \"timestamp\": \"2025-10-23T14:49:53.099429Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "{\"session_id\": \"session_20251023_184945_7e656532\", \"user_input\": \"How often are most respondents updating their models?\", \"answer_preview\": \"More than 50% of organizations update their models at least monthly, with 17% performing updates on a weekly basis. This aggressive update schedule re\", \"timestamp\": \"2025-10-23T14:49:53.099429Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised NotFound: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised NotFound: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods..\n",
      "3it [00:38, 12.94s/it]\n",
      "3it [00:38, 12.94s/it]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "#  Bydefault LangChainStringEvaluator use OpenAI  so below code will give an error without OpenAI API key.\n",
    "# Evaluators\n",
    "qa_evaluator = [LangChainStringEvaluator(\"cot_qa\")]\n",
    "dataset_name = \"LLMOPS_Dataset\"\n",
    "\n",
    "# Run evaluation using our RAG function\n",
    "experiment_results = evaluate(\n",
    "    answer_ai_report_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"test-agenticAIReport-qa-rag\",\n",
    "    # Experiment metadata\n",
    "    metadata={\n",
    "        \"variant\": \"RAG with FAISS and AI Engineering Report\",\n",
    "        \"chunk_size\": 1000,\n",
    "        \"chunk_overlap\": 200,\n",
    "        \"k\": 5,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ab45b3",
   "metadata": {},
   "source": [
    "## Custom Correctness Evaluator\n",
    "\n",
    "Creating an LLM-as-a-Judge evaluator to assess semantic and factual alignment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd261713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def correctness_evaluator(run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    Custom LLM-as-a-Judge evaluator for correctness.\n",
    "    \n",
    "    Correctness means how well the actual model output matches the reference output \n",
    "    in terms of factual accuracy, coverage, and meaning.\n",
    "    \n",
    "    Args:\n",
    "        run: The Run object containing the actual outputs\n",
    "        example: The Example object containing the expected outputs\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'score' (1 for correct, 0 for incorrect) and 'reasoning'\n",
    "    \"\"\"\n",
    "    # Extract actual and expected outputs\n",
    "    actual_output = run.outputs.get(\"answer\", \"\")\n",
    "    expected_output = example.outputs.get(\"answer\", \"\")\n",
    "    input_question = example.inputs.get(\"question\", \"\")\n",
    "    \n",
    "    # Define the evaluation prompt\n",
    "    eval_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are an evaluator whose job is to judge correctness.\n",
    "\n",
    "Correctness means how well the actual model output matches the reference output in terms of factual accuracy, coverage, and meaning.\n",
    "\n",
    "- If the actual output matches the reference output semantically (even if wording differs), it should be marked correct.\n",
    "- If the output misses key facts, introduces contradictions, or is factually incorrect, it should be marked incorrect.\n",
    "\n",
    "Do not penalize for stylistic or formatting differences unless they change meaning.\"\"\"),\n",
    "        (\"human\", \"\"\"<example>\n",
    "<input>\n",
    "{input}\n",
    "</input>\n",
    "\n",
    "<output>\n",
    "Expected Output: {expected_output}\n",
    "\n",
    "Actual Output: {actual_output}\n",
    "</output>\n",
    "</example>\n",
    "\n",
    "Please grade the following agent run given the input, expected output, and actual output.\n",
    "Focus only on correctness (semantic and factual alignment).\n",
    "\n",
    "Respond with:\n",
    "1. A brief reasoning (1-2 sentences)\n",
    "2. A final verdict: either \"CORRECT\" or \"INCORRECT\"\n",
    "\n",
    "Format your response as:\n",
    "Reasoning: [your reasoning]\n",
    "Verdict: [CORRECT or INCORRECT]\"\"\")\n",
    "    ])\n",
    "    \n",
    "    # Initialize LLM (using Gemini as shown in your config)\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-pro\",\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Create chain and invoke\n",
    "    chain = eval_prompt | llm\n",
    "    \n",
    "    try:\n",
    "        response = chain.invoke({\n",
    "            \"input\": input_question,\n",
    "            \"expected_output\": expected_output,\n",
    "            \"actual_output\": actual_output\n",
    "        })\n",
    "        \n",
    "        response_text = response.content\n",
    "        \n",
    "        # Parse the response\n",
    "        reasoning = \"\"\n",
    "        verdict = \"\"\n",
    "        \n",
    "        for line in response_text.split('\\n'):\n",
    "            if line.startswith(\"Reasoning:\"):\n",
    "                reasoning = line.replace(\"Reasoning:\", \"\").strip()\n",
    "            elif line.startswith(\"Verdict:\"):\n",
    "                verdict = line.replace(\"Verdict:\", \"\").strip()\n",
    "        \n",
    "        # Convert verdict to score (1 for correct, 0 for incorrect)\n",
    "        score = 1 if \"CORRECT\" in verdict.upper() else 0\n",
    "        \n",
    "        return {\n",
    "            \"key\": \"correctness\",\n",
    "            \"score\": score,\n",
    "            \"reasoning\": reasoning,\n",
    "            \"comment\": f\"Verdict: {verdict}\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"key\": \"correctness\",\n",
    "            \"score\": 0,\n",
    "            \"reasoning\": f\"Error during evaluation: {str(e)}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18f628f",
   "metadata": {},
   "source": [
    "## Run Evaluation with Custom Correctness Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "516b20cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'agenticAIReport-correctness-eval-d0c3e72e' at:\n",
      "https://smith.langchain.com/o/5ab3a087-cdc4-52b4-b7ec-104866d876b9/datasets/a27d83f2-be8c-49f5-9106-6f38cb260c7d/compare?selectedSessions=76c6d724-0dba-4103-a0ab-65e84c55199a\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]{\"timestamp\": \"2026-01-04T13:47:25.932422Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:25.933200Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:25.933781Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:47:25.934343Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:25.932422Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:25.933200Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:25.933781Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:47:25.934343Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:47:25.945040Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20260104_174725_2802dfb1\", \"temp_dir\": \"data/session_20260104_174725_2802dfb1\", \"faiss_dir\": \"faiss_index/session_20260104_174725_2802dfb1\", \"sessionized\": true, \"timestamp\": \"2026-01-04T13:47:25.947883Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"The 2025 AI Engineering Report.txt\", \"saved_as\": \"data/session_20260104_174725_2802dfb1/a7a73a64.txt\", \"timestamp\": \"2026-01-04T13:47:25.949530Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2026-01-04T13:47:25.950269Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 10, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2026-01-04T13:47:25.950812Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:47:25.951331Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:47:25.945040Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20260104_174725_2802dfb1\", \"temp_dir\": \"data/session_20260104_174725_2802dfb1\", \"faiss_dir\": \"faiss_index/session_20260104_174725_2802dfb1\", \"sessionized\": true, \"timestamp\": \"2026-01-04T13:47:25.947883Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"The 2025 AI Engineering Report.txt\", \"saved_as\": \"data/session_20260104_174725_2802dfb1/a7a73a64.txt\", \"timestamp\": \"2026-01-04T13:47:25.949530Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2026-01-04T13:47:25.950269Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 10, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2026-01-04T13:47:25.950812Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:47:25.951331Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20260104_174725_2802dfb1\", \"timestamp\": \"2026-01-04T13:47:29.322674Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2026-01-04T13:47:29.323209Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:29.324273Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:29.324435Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:29.324584Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:47:29.324824Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:47:29.326092Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"groq\", \"model\": \"openai/gpt-oss-20b\", \"timestamp\": \"2026-01-04T13:47:29.326339Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20260104_174725_2802dfb1\", \"timestamp\": \"2026-01-04T13:47:29.322674Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2026-01-04T13:47:29.323209Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:29.324273Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:29.324435Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:29.324584Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:47:29.324824Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:47:29.326092Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"groq\", \"model\": \"openai/gpt-oss-20b\", \"timestamp\": \"2026-01-04T13:47:29.326339Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20260104_174725_2802dfb1\", \"timestamp\": \"2026-01-04T13:47:29.372585Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20260104_174725_2802dfb1\", \"timestamp\": \"2026-01-04T13:47:29.373213Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:29.375360Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:29.376083Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:29.376610Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:47:29.377142Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:47:29.380799Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:47:29.381827Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "{\"session_id\": \"session_20260104_174725_2802dfb1\", \"timestamp\": \"2026-01-04T13:47:29.372585Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20260104_174725_2802dfb1\", \"timestamp\": \"2026-01-04T13:47:29.373213Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:29.375360Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:29.376083Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:29.376610Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:47:29.377142Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:47:29.380799Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:47:29.381827Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"session_id\": \"session_20260104_174725_2802dfb1\", \"timestamp\": \"2026-01-04T13:47:32.484726Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20260104_174725_2802dfb1\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20260104_174725_2802dfb1\", \"timestamp\": \"2026-01-04T13:47:32.485247Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "{\"session_id\": \"session_20260104_174725_2802dfb1\", \"timestamp\": \"2026-01-04T13:47:32.484726Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20260104_174725_2802dfb1\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20260104_174725_2802dfb1\", \"timestamp\": \"2026-01-04T13:47:32.485247Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20260104_174725_2802dfb1\", \"user_input\": \"For customer-facing applications, which company's models dominate the top rankings?\", \"answer_preview\": \"OpenAI\\u2019s models dominate the top rankings for customer\\u2011facing applications.\", \"timestamp\": \"2026-01-04T13:47:33.589657Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20260104_174725_2802dfb1\", \"user_input\": \"For customer-facing applications, which company's models dominate the top rankings?\", \"answer_preview\": \"OpenAI\\u2019s models dominate the top rankings for customer\\u2011facing applications.\", \"timestamp\": \"2026-01-04T13:47:33.589657Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767534453.632712  505357 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767534453.632712  505357 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\n",
      "Please retry in 24.228906122s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerDay-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 24\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\n",
      "Please retry in 24.228906122s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerDay-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 24\n",
      "}\n",
      "].\n",
      "1it [00:12, 12.07s/it]{\"timestamp\": \"2026-01-04T13:47:38.004208Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:38.004748Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:38.005124Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:47:38.005479Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:47:38.007600Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20260104_174738_34e9f3d5\", \"temp_dir\": \"data/session_20260104_174738_34e9f3d5\", \"faiss_dir\": \"faiss_index/session_20260104_174738_34e9f3d5\", \"sessionized\": true, \"timestamp\": \"2026-01-04T13:47:38.008739Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"The 2025 AI Engineering Report.txt\", \"saved_as\": \"data/session_20260104_174738_34e9f3d5/709ea017.txt\", \"timestamp\": \"2026-01-04T13:47:38.009880Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2026-01-04T13:47:38.010455Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 10, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2026-01-04T13:47:38.010916Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:47:38.011286Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "1it [00:12, 12.07s/it]{\"timestamp\": \"2026-01-04T13:47:38.004208Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:38.004748Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:38.005124Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:47:38.005479Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:47:38.007600Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20260104_174738_34e9f3d5\", \"temp_dir\": \"data/session_20260104_174738_34e9f3d5\", \"faiss_dir\": \"faiss_index/session_20260104_174738_34e9f3d5\", \"sessionized\": true, \"timestamp\": \"2026-01-04T13:47:38.008739Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"The 2025 AI Engineering Report.txt\", \"saved_as\": \"data/session_20260104_174738_34e9f3d5/709ea017.txt\", \"timestamp\": \"2026-01-04T13:47:38.009880Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2026-01-04T13:47:38.010455Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 10, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2026-01-04T13:47:38.010916Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:47:38.011286Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20260104_174738_34e9f3d5\", \"timestamp\": \"2026-01-04T13:47:41.644136Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2026-01-04T13:47:41.644611Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:41.645790Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:41.646017Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:41.646238Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:47:41.646520Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:47:41.648217Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"groq\", \"model\": \"openai/gpt-oss-20b\", \"timestamp\": \"2026-01-04T13:47:41.648526Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20260104_174738_34e9f3d5\", \"timestamp\": \"2026-01-04T13:47:41.644136Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2026-01-04T13:47:41.644611Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:41.645790Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:41.646017Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:41.646238Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:47:41.646520Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:47:41.648217Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"groq\", \"model\": \"openai/gpt-oss-20b\", \"timestamp\": \"2026-01-04T13:47:41.648526Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20260104_174738_34e9f3d5\", \"timestamp\": \"2026-01-04T13:47:41.680693Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20260104_174738_34e9f3d5\", \"timestamp\": \"2026-01-04T13:47:41.681114Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:41.682118Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:41.682288Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:41.682439Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:47:41.682584Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:47:41.683903Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:47:41.684062Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"session_id\": \"session_20260104_174738_34e9f3d5\", \"timestamp\": \"2026-01-04T13:47:41.680693Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20260104_174738_34e9f3d5\", \"timestamp\": \"2026-01-04T13:47:41.681114Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:41.682118Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:41.682288Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:41.682439Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:47:41.682584Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:47:41.683903Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:47:41.684062Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"session_id\": \"session_20260104_174738_34e9f3d5\", \"timestamp\": \"2026-01-04T13:47:44.798043Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20260104_174738_34e9f3d5\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20260104_174738_34e9f3d5\", \"timestamp\": \"2026-01-04T13:47:44.798489Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "{\"session_id\": \"session_20260104_174738_34e9f3d5\", \"timestamp\": \"2026-01-04T13:47:44.798043Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20260104_174738_34e9f3d5\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20260104_174738_34e9f3d5\", \"timestamp\": \"2026-01-04T13:47:44.798489Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20260104_174738_34e9f3d5\", \"user_input\": \"What percentage of respondents are using RAG in some form?\", \"answer_preview\": \"70% of respondents are using RAG in some form.\", \"timestamp\": \"2026-01-04T13:47:45.436669Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20260104_174738_34e9f3d5\", \"user_input\": \"What percentage of respondents are using RAG in some form?\", \"answer_preview\": \"70% of respondents are using RAG in some form.\", \"timestamp\": \"2026-01-04T13:47:45.436669Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "E0000 00:00:1767534465.443950  505357 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1767534465.443950  505357 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\n",
      "Please retry in 14.102047068s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerDay-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 14\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\n",
      "Please retry in 14.102047068s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerDay-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 14\n",
      "}\n",
      "].\n",
      "2it [00:22, 10.92s/it]{\"timestamp\": \"2026-01-04T13:47:48.109338Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:48.110089Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:48.110566Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:47:48.111056Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:47:48.113771Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20260104_174748_bb7e750c\", \"temp_dir\": \"data/session_20260104_174748_bb7e750c\", \"faiss_dir\": \"faiss_index/session_20260104_174748_bb7e750c\", \"sessionized\": true, \"timestamp\": \"2026-01-04T13:47:48.114854Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"The 2025 AI Engineering Report.txt\", \"saved_as\": \"data/session_20260104_174748_bb7e750c/ef6246e3.txt\", \"timestamp\": \"2026-01-04T13:47:48.116019Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2026-01-04T13:47:48.116664Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 10, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2026-01-04T13:47:48.117171Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:47:48.117686Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "2it [00:22, 10.92s/it]{\"timestamp\": \"2026-01-04T13:47:48.109338Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:48.110089Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:48.110566Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:47:48.111056Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:47:48.113771Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20260104_174748_bb7e750c\", \"temp_dir\": \"data/session_20260104_174748_bb7e750c\", \"faiss_dir\": \"faiss_index/session_20260104_174748_bb7e750c\", \"sessionized\": true, \"timestamp\": \"2026-01-04T13:47:48.114854Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"The 2025 AI Engineering Report.txt\", \"saved_as\": \"data/session_20260104_174748_bb7e750c/ef6246e3.txt\", \"timestamp\": \"2026-01-04T13:47:48.116019Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 1, \"timestamp\": \"2026-01-04T13:47:48.116664Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 10, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2026-01-04T13:47:48.117171Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:47:48.117686Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20260104_174748_bb7e750c\", \"timestamp\": \"2026-01-04T13:47:51.315972Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2026-01-04T13:47:51.316322Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:51.317187Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:51.317353Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:51.317524Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:47:51.317733Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:47:51.318930Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"groq\", \"model\": \"openai/gpt-oss-20b\", \"timestamp\": \"2026-01-04T13:47:51.319063Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"added\": 1, \"index\": \"faiss_index/session_20260104_174748_bb7e750c\", \"timestamp\": \"2026-01-04T13:47:51.315972Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2026-01-04T13:47:51.316322Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:51.317187Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:51.317353Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:51.317524Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:47:51.317733Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:47:51.318930Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"groq\", \"model\": \"openai/gpt-oss-20b\", \"timestamp\": \"2026-01-04T13:47:51.319063Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20260104_174748_bb7e750c\", \"timestamp\": \"2026-01-04T13:47:51.344800Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20260104_174748_bb7e750c\", \"timestamp\": \"2026-01-04T13:47:51.345222Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:51.346189Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:51.346413Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:51.346616Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:47:51.346765Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:47:51.347916Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:47:51.348087Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"session_id\": \"session_20260104_174748_bb7e750c\", \"timestamp\": \"2026-01-04T13:47:51.344800Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20260104_174748_bb7e750c\", \"timestamp\": \"2026-01-04T13:47:51.345222Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:51.346189Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:51.346413Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2026-01-04T13:47:51.346616Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_1b...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2026-01-04T13:47:51.346765Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-04T13:47:51.347916Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"huggingface\", \"model\": \"all-MiniLM-L6-v2\", \"timestamp\": \"2026-01-04T13:47:51.348087Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "{\"session_id\": \"session_20260104_174748_bb7e750c\", \"timestamp\": \"2026-01-04T13:47:54.580772Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20260104_174748_bb7e750c\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20260104_174748_bb7e750c\", \"timestamp\": \"2026-01-04T13:47:54.581285Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "{\"session_id\": \"session_20260104_174748_bb7e750c\", \"timestamp\": \"2026-01-04T13:47:54.580772Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20260104_174748_bb7e750c\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20260104_174748_bb7e750c\", \"timestamp\": \"2026-01-04T13:47:54.581285Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20260104_174748_bb7e750c\", \"user_input\": \"How often are most respondents updating their models?\", \"answer_preview\": \"Most respondents update their models on a monthly basis.\", \"timestamp\": \"2026-01-04T13:47:56.198496Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20260104_174748_bb7e750c\", \"user_input\": \"How often are most respondents updating their models?\", \"answer_preview\": \"Most respondents update their models on a monthly basis.\", \"timestamp\": \"2026-01-04T13:47:56.198496Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "E0000 00:00:1767534476.206626  505357 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1767534476.206626  505357 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\n",
      "Please retry in 3.330778387s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerDay-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 3\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\n",
      "Please retry in 3.330778387s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerDay-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 3\n",
      "}\n",
      "].\n",
      "3it [00:33, 11.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation completed! Check the LangSmith UI for detailed results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation with the custom correctness evaluator\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# Define evaluators - using custom correctness evaluator\n",
    "evaluators = [correctness_evaluator]\n",
    "\n",
    "dataset_name = \"LLMOPS_Dataset\"\n",
    "\n",
    "# Run evaluation\n",
    "experiment_results = evaluate(\n",
    "    answer_ai_report_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=evaluators,\n",
    "    experiment_prefix=\"agenticAIReport-correctness-eval\",\n",
    "    description=\"Evaluating RAG system with custom correctness evaluator (LLM-as-a-Judge)\",\n",
    "    metadata={\n",
    "        \"variant\": \"RAG with FAISS and AI Engineering Report\",\n",
    "        \"evaluator\": \"custom_correctness_llm_judge\",\n",
    "        \"model\": \"gemini-2.5-pro\",\n",
    "        \"chunk_size\": 1000,\n",
    "        \"chunk_overlap\": 200,\n",
    "        \"k\": 5,\n",
    "    },\n",
    ")\n",
    "print(\"\\nEvaluation completed! Check the LangSmith UI for detailed results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a98671e",
   "metadata": {},
   "source": [
    "## Optional: Combine Multiple Evaluators\n",
    "You can use multiple evaluators together to get different perspectives on your RAG system's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3151570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Combine custom correctness evaluator with LangChain's built-in evaluators\n",
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "# Combine custom and built-in evaluators\n",
    "combined_evaluators = [\n",
    "    correctness_evaluator,  # Custom LLM-as-a-Judge\n",
    "    LangChainStringEvaluator(\"cot_qa\"),  # Chain-of-thought QA evaluator\n",
    "]\n",
    "\n",
    "# Run evaluation with multiple evaluators\n",
    "# Uncomment to run:\n",
    "# experiment_results_combined = evaluate(\n",
    "#     answer_ai_report_question,\n",
    "#     data=dataset_name,\n",
    "#     evaluators=combined_evaluators,\n",
    "#     experiment_prefix=\"agenticAIReport-multi-eval\",\n",
    "#     description=\"Evaluating RAG system with multiple evaluators\",\n",
    "#     metadata={\n",
    "#         \"variant\": \"RAG with FAISS\",\n",
    "#         \"evaluators\": \"correctness + cot_qa\",\n",
    "#         \"chunk_size\": 1000,\n",
    "#         \"chunk_overlap\": 200,\n",
    "#         \"k\": 5,\n",
    "#     },\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
